{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f717f0-4638-405e-9513-5c7ad0557bd4",
   "metadata": {},
   "source": [
    "# Naukri WebScraping Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ea3a47-c75a-4cff-895e-199f03e14542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "chrome_path = r\"C:\\Users\\salma\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "chrome_service = webdriver.ChromeService(executable_path=chrome_path)\n",
    "driver = webdriver.Chrome(service = chrome_service)\n",
    "# print('Done')\n",
    "url = '''https://www.naukri.com/data-analyst-jobs-in-noida'''\n",
    "# url = '''https://www.bloomberg.com/billionaires/'''\n",
    "driver.get(url)\n",
    "# time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32f22ee-88b4-43b6-8913-65fd534d4e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "html_data = driver.page_source\n",
    "driver.close()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41230282-2fd1-4194-b480-ff78ac417e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_data)\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef0aaf-a50a-4027-8e28-19ed88e0c9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900c06e-778d-432f-b4f2-88e29fa2119b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f888575-723c-4444-b555-7a6c4fd6c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_div_cards = soup.find_all('div', {'class':'cust-job-tuple layout-wrapper lay-2 sjw__tuple'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35ede7db-9b3a-4ac5-bc89-9e3e65df57aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_div_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2129016e-ae74-4d8a-b845-625f30531b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_type = 'data-analyst'\n",
    "# loc = 'pune'\n",
    "# page_no = 3\n",
    "# f'''https://www.naukri.com/{job_type}-jobs-in-{loc}-{page_no}'''.split('-')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8b43df3-f2c8-466c-a994-2cea59c227e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_post_page = []\n",
    "def scrap_naukri_page(loc = 'noida',page_no = 0, job_type = 'data-analyst'):\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import requests as r\n",
    "    from bs4 import BeautifulSoup\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "    chrome_path = r\"C:\\Users\\salma\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "    \n",
    "    chrome_service = webdriver.ChromeService(executable_path=chrome_path)\n",
    "    driver = webdriver.Chrome(service = chrome_service)\n",
    "    # print('Done')\n",
    "    url = f'''https://www.naukri.com/{job_type}-jobs-in-{loc}-{page_no}'''\n",
    "    print(url)\n",
    "    # url = '''https://www.bloomberg.com/billionaires/'''\n",
    "    driver.get(url)\n",
    "    time.sleep(30)\n",
    "    \n",
    "    \n",
    "    html_data = driver.page_source\n",
    "    driver.close()\n",
    "    # print('Done')\n",
    "    \n",
    "    soup = BeautifulSoup(html_data)\n",
    "    \n",
    "    all_div_cards = soup.find_all('div', {'class':'cust-job-tuple layout-wrapper lay-2 sjw__tuple'})\n",
    "    \n",
    "    \n",
    "    all_job_title = []\n",
    "    location = [url.split('-')[-2]]*len(all_div_cards)\n",
    "    all_job_sub_apply_link = []\n",
    "    all_company_name = []\n",
    "    all_company_listed_jobs_link = []\n",
    "    all_user_rating = []\n",
    "    all_user_reviews= []\n",
    "    all_min_exp = []\n",
    "    all_max_exp = []\n",
    "    all_job_desc = []\n",
    "    all_jd_tags = []\n",
    "    all_jpd_days_mark = []\n",
    "    all_date_extn = []\n",
    "    all_company_logo_img = []\n",
    "    scraped_date_time = []\n",
    "    pg_no = [page_no]*len(all_div_cards)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    c = 1\n",
    "    for div in all_div_cards:\n",
    "        \n",
    "        for a_tag in div.find_all('a',class_ = 'title',href = True):\n",
    "            \n",
    "            job_title, sub_apply_link = a_tag.text,a_tag['href']\n",
    "            \n",
    "            all_job_title.append(job_title.strip())\n",
    "            all_job_sub_apply_link.append(sub_apply_link.strip())\n",
    "\n",
    "        # Company Name scrap\n",
    "        for j in div.find_all('span',class_ = 'comp-dtls-wrap'):\n",
    "            for k in j.find_all('a',class_ = 'comp-name'):\n",
    "                # company_name, company_listed_jobs_link = k.text,k['href']\n",
    "                company_name = k.text\n",
    "                all_company_name.append(company_name.strip())\n",
    "                # print(c,company_name.strip())\n",
    "                # print(company_name,end = '.')\n",
    "                try:\n",
    "                    div.find_all('span',class_ = 'main-2')[0].text\n",
    "                except:\n",
    "                    all_user_rating.append(0)\n",
    "                    all_user_reviews.append(0)\n",
    "                    \n",
    "                # all_company_listed_jobs_link.append(company_listed_jobs_link.strip())\n",
    "                \n",
    "                # c += 1\n",
    "            for l in j.find_all('span',class_ = 'main-2'):\n",
    "                # print(div.find_all('span',class_ = 'main-2'))\n",
    "                user_rating = l.text.strip()\n",
    "                try:\n",
    "                    all_user_rating.append(float(user_rating))\n",
    "                except:\n",
    "                    all_user_rating.append(user_rating)\n",
    "                \n",
    "            for m in j.find_all('a',class_ = 'review ver-line',title = 'Powered by Ambition Box'):\n",
    "                user_reviews = m.text.upper().replace('Reviews'.upper(),'').strip()\n",
    "                all_user_reviews.append(user_reviews)\n",
    "                \n",
    "        for exp in div.find_all('span',class_ = 'exp-wrap'):\n",
    "            min_max_exp = exp.text.upper().replace('YRS',\"\").strip()\n",
    "            # print('ERRR',min_max_exp.split('-'))\n",
    "            try:\n",
    "                min_exp, max_exp = min_max_exp.split('-')\n",
    "            except:\n",
    "                min_exp, max_exp = [0,0]\n",
    "    \n",
    "            all_min_exp.append(min_exp)\n",
    "            all_max_exp.append(max_exp)\n",
    "            # print(min_exp,\"<>\",max_exp)\n",
    "    \n",
    "        for jd in div.find_all('span',class_ = 'job-desc ni-job-tuple-icon ni-job-tuple-icon-srp-description'):\n",
    "            job_desc = jd.text.strip()\n",
    "            all_job_desc.append(job_desc)\n",
    "            \n",
    "            # c +=1\n",
    "        for job_tags in div.find_all('ul',class_ = 'tags-gt'):\n",
    "            temp_tags = []\n",
    "            for each_tags in job_tags.find_all('li'):\n",
    "                tg = each_tags.text.strip()\n",
    "                temp_tags.append(tg)\n",
    "            \n",
    "            all_jd_tags.append(temp_tags)\n",
    "            # c +=1 \n",
    "            # temp_tags = []\n",
    "            # print('-----------------------')\n",
    "        for job_post_date in div.find_all('span',class_ = 'job-post-day'):\n",
    "            jpd_num, jpd_days_extn = job_post_date.text.upper().replace('AGO','').replace(\"+\",'').strip().split(' ')\n",
    "    \n",
    "            all_jpd_days_mark.append(jpd_num)\n",
    "            \n",
    "            all_date_extn.append(jpd_days_extn)\n",
    "        \n",
    "        for img_span in div.find_all('span',class_ = 'imagewrap'):\n",
    "            for img in img_span.find_all('img',src = True):\n",
    "                company_logo_img = img['src']\n",
    "                all_company_logo_img.append(company_logo_img)\n",
    "    \n",
    "    \n",
    "    sp_time = time.asctime()\n",
    "    scraped_date_time.extend([sp_time]*len(all_div_cards))\n",
    "\n",
    "    table = {'Naukri-Page-No':pg_no,\n",
    "            'all_job_title':all_job_title,\n",
    "            'location':location,\n",
    "            'all_job_sub_apply_link':all_job_sub_apply_link,\n",
    "            'all_company_name':all_company_name,\n",
    "            # 'all_company_listed_jobs_link':all_company_listed_jobs_link,\n",
    "            'all_user_rating':all_user_rating,\n",
    "            'all_user_reviews':all_user_reviews,\n",
    "            'all_min_exp':all_min_exp,\n",
    "            'all_max_exp':all_max_exp,\n",
    "            'all_job_desc':all_job_desc,\n",
    "            'all_jd_tags':all_jd_tags,\n",
    "            'all_jpd_days_mark':all_jpd_days_mark,\n",
    "            'all_date_extn':all_date_extn,\n",
    "            'all_company_logo_img':all_company_logo_img,\n",
    "            'scraped_date_time':scraped_date_time}\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # for i in table:\n",
    "        #     print(i, len(table[i]))\n",
    "        return url,pd.DataFrame(table)\n",
    "        \n",
    "    except Exception as err:\n",
    "        print('Error: ',err)\n",
    "        for i in table:\n",
    "            print(i, len(table[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50b2cb-ce35-437e-9b3a-a9e1ea5cdbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207752f5-2399-4727-9f98-3d61ccfa44b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d813b42-7148-4bf9-b957-2a9e5db2b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     try:\n",
    "#         table = scrap_naukri_page('noida',page_no=i,job_type='Data-Analyst')\n",
    "#         url,df = table\n",
    "        \n",
    "#         folder_path = r\"C:\\Users\\salma\\OneDrive\\Desktop\\Naukri_all_jobs\"\n",
    "#         file_name = url.split('/')[-1]  #.split('-')[:-1])\n",
    "        \n",
    "#         file_path = folder_path + '\\\\' + file_name + '.xlsx'\n",
    "#         df.to_excel(file_path)\n",
    "#         print(f'file: {i+1}: Done')\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "    \n",
    "# # import os\n",
    "# # os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b85a9039-c32c-4532-b044-2baba6afa019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_excel(func,l,p,j):\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        table = func(l,page_no=p,job_type=j)\n",
    "        url,df = table\n",
    "        \n",
    "        folder_path = r\"C:\\Users\\salma\\OneDrive\\Desktop\\Naukri_all_jobs\"\n",
    "        file_name = url.split('/')[-1]  #.split('-')[:-1])\n",
    "        \n",
    "        file_path = folder_path + '\\\\' + file_name + '.xlsx'\n",
    "        df.to_excel(file_path)\n",
    "        print(f'file: Done')\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2952f89-12d4-4736-b0cc-87c7f1bffccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_excel(scrap_naukri_page,'noida',0,'Data-Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09dc9c68-4cb3-4d5d-9473-39509cee0bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-3\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-1\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-2\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-8\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-6\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-9\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-0\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-5\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-4\n",
      "https://www.naukri.com/Data-Analyst-jobs-in-noida-7\n",
      "file: Done\n",
      "file: Done\n",
      "file: Done\n",
      "file: Done\n",
      "file: Done\n",
      "file: Done\n",
      "Error:  All arrays must be of the same length\n",
      "Naukri-Page-No 20\n",
      "all_job_title 20\n",
      "location 20\n",
      "all_job_sub_apply_link 20\n",
      "all_company_name 20\n",
      "all_user_rating 20\n",
      "all_user_reviews 20\n",
      "all_min_exp 20\n",
      "all_max_exp 20\n",
      "all_job_desc 20\n",
      "all_jd_tags 20\n",
      "all_jpd_days_mark 20\n",
      "all_date_extn 20\n",
      "all_company_logo_img 19\n",
      "scraped_date_time 20\n",
      "file: Done\n"
     ]
    }
   ],
   "source": [
    "import threading as th\n",
    "all_thread = []\n",
    "for i in range(10):\n",
    "    t = th.Thread(target = data_to_excel, args=[scrap_naukri_page,'noida',i,'Data-Analyst'])\n",
    "    all_thread.append(t)\n",
    "\n",
    "for k in all_thread:\n",
    "    k.start()\n",
    "\n",
    "for j in all_thread:\n",
    "    j.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2675ec-7a7e-4654-9573-d7a4aa479644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d74b5a-afe4-4b71-ad19-7091d89858cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
